{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Template",
      "provenance": [],
      "collapsed_sections": [
        "tI0OA-B3xtCU",
        "C0Aa77Wd16Lf",
        "fJK1FBXF2HZA",
        "IsABbVS16JfR",
        "EOFkigUk5338",
        "kCpRCFC76eH8",
        "DmR5MwIg7RlA",
        "Iqdut7JO7nEY",
        "MFZ56U2Z8TmM",
        "HfvcKcZu8Y-R",
        "Z3K4aAs2ACWs",
        "cG_mDf55BsXV",
        "1M0XeFOuDFOn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoI6_uhWwblM"
      },
      "source": [
        "**< Title >**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPchv7bYwikT"
      },
      "source": [
        "**We're going to take the following approach:**\r\n",
        ">1. Problem Defintion\r\n",
        "2. Data\r\n",
        "3. Evaluation\r\n",
        "4. Features\r\n",
        "5. Modelling\r\n",
        "6. Experimentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfwY0Ycywoat"
      },
      "source": [
        "# ***1. Problem Defintion***\r\n",
        "\r\n",
        "In a statement,\r\n",
        ">***Explain the problem***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkvbgWw8wuun"
      },
      "source": [
        "# ***2. Data***\r\n",
        "\r\n",
        "*The original data came from < The dataset title> data from the < The website/company title >.*\r\n",
        "\r\n",
        "< Dataset url >\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z08QJ6tCxSfh"
      },
      "source": [
        "## ***3. Evaluation***\r\n",
        "\r\n",
        "*< Explain what you want to achieve>*\r\n",
        "\r\n",
        "< Evaluation URL > \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI0OA-B3xtCU"
      },
      "source": [
        "# ***4. Features***\r\n",
        "\r\n",
        "*Some information about the data:*\r\n",
        "\r\n",
        "*   \r\n",
        "*   \r\n",
        "*   \r\n",
        "*   \r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd8Pr8okx8d2"
      },
      "source": [
        "# Import all the tools we need\r\n",
        "\r\n",
        "# Regular EDA (Exploratory data analysis) and plotting libraries\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "# Models from Scikit-Learn(Regression)\r\n",
        "from sklearn.linear_model import LinearRegression \r\n",
        "from sklearn.preprocessing import PolynomialFeatures\r\n",
        "from sklearn.svm import SVR\r\n",
        "from skelearn.tree import DecisionTreeRegressor\r\n",
        "from skelarn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "# Models from Scikit-Learn(Classification)\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from skelearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "# Model Evaluations(Regression)\r\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\r\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\r\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "# Model Evaluations(Classification)\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\r\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\r\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Aa77Wd16Lf"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yLWje9F16_2"
      },
      "source": [
        "file_path = < File Path >\r\n",
        "df = pd.read_csv(file_path)\r\n",
        "df.shape # (rows, columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJK1FBXF2HZA"
      },
      "source": [
        "## Data Exploration (Exploratory data analysis or EDA)\r\n",
        "\r\n",
        "The goal here is to find out more about the data and become a subject matter export on the dataset you're working with.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "1.   What question(s) are you trying to solve?\r\n",
        "2.   What kind of data do we have and how do we treat different types?\r\n",
        "3.   What's missing from the data and how do you deal with it?\r\n",
        "4.   Where are the outliers and why should you care about them?\r\n",
        "5.   How can you add, change or remove features to get more out of your data?\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m6JU5-j2Jwr"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzitcz0m2MN6"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m__KOILi2M_Z"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVLZZ0pL2PAk"
      },
      "source": [
        "df.info();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BJvP4WM2WDd"
      },
      "source": [
        "# Let's find out how many of each feature there\r\n",
        "df[\"<Target>\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCfn0jSF2YRw"
      },
      "source": [
        "# Are there any missing values?\r\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaHCwG9z38gd"
      },
      "source": [
        "# How many images are there of each label?(Classification)\r\n",
        "labels_csv.<target>.value_counts().plot.bar(figsize=(20,10));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHN9G2Bv4G46"
      },
      "source": [
        "# What's the median number of labels per class?(Classification)\r\n",
        "labels_csv.breed.value_counts().median()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rRtngn-4eKs"
      },
      "source": [
        "# Compare target column with <feature> column\r\n",
        "pd.crosstab(df.<target>, df.<feature>)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9KvMhK64j-b"
      },
      "source": [
        "# Create a plot of crosstab\r\n",
        "pd.crosstab(df.<target>,df.<feature>).plot(kind=\"bar\",color=[\"salmon\",\"lightblue\"])\r\n",
        "plt.title(\"\")\r\n",
        "plt.xlabel(\"<Feature labels>\")\r\n",
        "plt.ylabel(\"Amount\")\r\n",
        "plt.legend([\"<Featur labels>]);\r\n",
        "plt.xticks(rotation=0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB8mGCE244Gf"
      },
      "source": [
        "# Correlation matrix a little prettier\r\n",
        "corr_matrix = df.corr()\r\n",
        "fix, ax = plt.subplots(figsize= (15,10))\r\n",
        "ax = sns.heatmap(corr_matrix, annot=True, fmt=\".2f\",cmap=\"YlGnBu\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsABbVS16JfR"
      },
      "source": [
        "## ***Make a copy of the original DataFrame***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bOEj9CC6K4V"
      },
      "source": [
        "#Make a copy\r\n",
        "df_tmp = df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOFkigUk5338"
      },
      "source": [
        "## ***Convert string to categories***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OAZtqfN54V6"
      },
      "source": [
        "# Find the columns which contain string\r\n",
        "for label, content in df_tmp.items():\r\n",
        "  if pd.api.types.is_string_dtype(content):\r\n",
        "    print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM8-2ZMw6BNs"
      },
      "source": [
        "# This will turn all of the string values into category values\r\n",
        "for label, content in df_tmp.items():\r\n",
        "  if pd.api.types.is_string_dtype(content):\r\n",
        "    df_tmp[label] = content.astype(\"category\").cat.as_ordered()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW1C-ONi6DEC"
      },
      "source": [
        "df_tmp.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCQ__rha6QCy"
      },
      "source": [
        "df_tmp.<Feature>.cat.categories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCkgmJ6c6UT7"
      },
      "source": [
        "df_tmp.<Feature>.cat.codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIOf65vE6XO1"
      },
      "source": [
        "#check missing data\r\n",
        "df_tmp.isnull().sum()/len(df_tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCpRCFC76eH8"
      },
      "source": [
        "## ***Save preprocessed data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsx1uiHp6k5Q"
      },
      "source": [
        "# Export current tmp dataframe\r\n",
        "df_tmp.to_csv(\"/content/train_tmp.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RdS_hGT7vgk"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85MWdd257wZb"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQdzvGdw7LRM"
      },
      "source": [
        "## ***Fill missing values***\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmR5MwIg7RlA"
      },
      "source": [
        "###   ***Fill numerical missing values***\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo--wMP97R5v"
      },
      "source": [
        "for label, content in df_tmp.items():\r\n",
        "  if pd.api.types.is_numeric_dtype(content):\r\n",
        "    print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJGiEYCf7cls"
      },
      "source": [
        "# Check for which numeric columns have null values\r\n",
        "for label, content in df_tmp.items():\r\n",
        "  if pd.api.types.is_numeric_dtype(content):\r\n",
        "    if pd.isnull(content).sum():\r\n",
        "      print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMO-L6pG7fGe"
      },
      "source": [
        "# Fill numeric rows with the median \r\n",
        "for label, content in df_tmp.items():\r\n",
        "  if pd.api.types.is_numeric_dtype(content):\r\n",
        "    if pd.isnull(content).sum():\r\n",
        "      # Add a binary column which tells us if the data was missing or not\r\n",
        "      df_tmp[label + \"_is_missing\"] = pd.isnull(content)\r\n",
        "      # Fill missing numeric values with median\r\n",
        "      df_tmp[label] = content.fillna(content.median())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYSFbgku7gZV"
      },
      "source": [
        "# Check if there is any null numeric values\r\n",
        "for labels, content in df_tmp.items():\r\n",
        "  if pd.api.types.is_numeric_dtype(content):\r\n",
        "    if pd.isnull(content).sum():\r\n",
        "      print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ1V0aLA7hnR"
      },
      "source": [
        "# Check to see how many examples were missing\r\n",
        "df_tmp.auctioneerID_is_missing.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OaC9SxL7lRr"
      },
      "source": [
        "df_tmp.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rddx-EGp7r3z"
      },
      "source": [
        "df_tmp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsoUI5a67tGK"
      },
      "source": [
        "df_tmp.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqdut7JO7nEY"
      },
      "source": [
        "### ***Fill and turning categorical variables into numbers***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WthXc5si7nd6"
      },
      "source": [
        "# Check for columns which aren't numeric\r\n",
        "for label, content in df_tmp.items():\r\n",
        "  if not pd.api.types.is_numeric_dtype(content):\r\n",
        "    print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H4Yjuu87p_9"
      },
      "source": [
        "# Turn categorical variables into numbers \r\n",
        "for label, content in df_tmp.items():\r\n",
        "  if not pd.api.types.is_numeric_dtype(content):\r\n",
        "    #Add a binary column to indicate whether sample had missing value\r\n",
        "    df_tmp[label + \"_is_missing\"] = pd.isnull(content)\r\n",
        "    #Turn categories into numbers and add +1\r\n",
        "    df_tmp[label] = pd.Categorical(content).codes + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIsDs-eB70ZA"
      },
      "source": [
        "df_tmp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUPOWsFM71gB"
      },
      "source": [
        "df_tmp.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFZ56U2Z8TmM"
      },
      "source": [
        "# ***Instantiate model***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfvcKcZu8Y-R"
      },
      "source": [
        "## ***Splitting data into train/validation sets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu4rdwW89IKe"
      },
      "source": [
        "# Split data into X & y \r\n",
        "df_X, df_y = df_tmp.drop(\"<Target>\", axis=1), df_tmp.<Target>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDzD4lkQ9XLn"
      },
      "source": [
        "# Split data into train and test sets\r\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(df_X,df_y,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MehO_fIB9sTS"
      },
      "source": [
        "## **Building an evaluation functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVYIiJQq9s0v"
      },
      "source": [
        "def evaluation_function(y_test,y_preds):\r\n",
        "  return <Evaluation_Target>(y_test,y_preds)\r\n",
        "\r\n",
        "# Create function to evaluate model on a few different levels(Regression)\r\n",
        "def show_scores(model):\r\n",
        "  train_preds = model.predict(X_train)\r\n",
        "  val_preds = model.predict(X_valid)\r\n",
        "  scores = {\"Training MAE\": mean_absolute_error(y_train,train_preds),\r\n",
        "            \"Valid MAE\": mean_absolute_error(y_valid,val_preds),\r\n",
        "            \"Training <Evaluation Target>\": evaluation_function(y_train, train_preds),\r\n",
        "            \"Valid <Evaluation Target>\": evaluation_function(y_valid, val_preds),\r\n",
        "            \"Training R^2\": r2_score(y_train, train_preds),\r\n",
        "            \"Valid R^2\": r2_score(y_valid, val_preds)}\r\n",
        "  return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DLaWuzo-aE1"
      },
      "source": [
        "# Create functions to evaluate model on a few different levels(Classification)\r\n",
        "def plot_conf_mat(y_test, y_preds):\r\n",
        "  \"\"\"\r\n",
        "  Plots a nice looking confusion matrix using Seaborn's heat map\r\n",
        "  \"\"\"\r\n",
        "  fig, ax = plt.subplots()\r\n",
        "  ax = sns.heatmap(confusion_matrix(y_test,y_preds),annot=True,cbar=False)\r\n",
        "  plt.xlabel(\"True Label\")\r\n",
        "  plt.ylabel(\"Predicted Label\")\r\n",
        "\r\n",
        "print(classification_report(y_test,y_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3K4aAs2ACWs"
      },
      "source": [
        "# ***Modelling***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGYdXlabADID"
      },
      "source": [
        "# Put models in a dictionary\r\n",
        "models = {\"<Model Name>\": <Model Function>(random_state=<Number>),\r\n",
        "          \"<Model Name>\": <Model Function>(random_state=<Number>)}\r\n",
        "\r\n",
        "# Create a function to fit and score models\r\n",
        "def fit_and_score(models,X_train,X_test,y_train,y_test):\r\n",
        "  \"\"\"\r\n",
        "  Fits and evaluates given machine learning models.\r\n",
        "  models : a dict of differetn Scikit-Learn machine learning models\r\n",
        "  X_train : training data (no labels)\r\n",
        "  X_test : testing data (no labels)\r\n",
        "  y_train : training labels\r\n",
        "  y_test : test labels\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  # Make a dictionary to keep model scores\r\n",
        "  model_scores = {}\r\n",
        "\r\n",
        "  #Loop through models\r\n",
        "  for name, model in models.items():\r\n",
        "    # Fit the model to the data\r\n",
        "    model.fit(X_train,y_train)\r\n",
        "    # Evaluate the model and append its score to model_scores\r\n",
        "    model_scores[name] = model.score(X_test, y_test)\r\n",
        "  return model_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC_rBcePBEjr"
      },
      "source": [
        "model_scores = fit_and_score(models, X_train,X_valid,y_train,y_valid)\r\n",
        "\r\n",
        "model_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SFzSX8jBGb_"
      },
      "source": [
        "model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\r\n",
        "model_compare.T.plot.bar();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG_mDf55BsXV"
      },
      "source": [
        "## **Hyperparameter tuning with RandomizedSearchCV**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE31ISFTBso8"
      },
      "source": [
        "# Different RandomForestRegressor hyperparameters\r\n",
        "rf_grid = {\"<Model Feature>\": <Model Feature Inputs>(One or many(If many [])),\r\n",
        "           \"<Model Feature>\": <Model Feature Inputs>(One or many(If many []))\r\n",
        "           }\r\n",
        "\r\n",
        "# Instantiate RandomizedSearchCV model\r\n",
        "rs_model = RandomizedSearchCV(RandomForestRegressor(n_jobs=-1, random_state=42),\r\n",
        "                              param_distributions=rf_grid,\r\n",
        "                              n_iter=2, # Number of parameter settings that are sampled\r\n",
        "                              cv=5, # Cross validation\r\n",
        "                              verbose=True) # Controls the verbosity: the higher, the more messages\r\n",
        "\r\n",
        "# Fit the RandomizedSearchCV model\r\n",
        "rs_model.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swF-C2nGEsoq"
      },
      "source": [
        "# Find the best hyperparameters\r\n",
        "rs_model.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DimXf38AEvTs"
      },
      "source": [
        "# Evaluate the randomized search best model\r\n",
        "rs_model.score(X_valid,y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M0XeFOuDFOn"
      },
      "source": [
        "## ***Hyperparamter Tuning with GridSearchCV***\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXJsuLJHDJYF"
      },
      "source": [
        "# Different hyperparameters for our best model\r\n",
        "model_grid = {\"<Model Feature>\": <Model Feature Inputs>(One or many(If many [])),\r\n",
        "                \"<Model Feature>\": <Model Feature Inputs>(One or many(If many []))}\r\n",
        "\r\n",
        "# Setup grid hyperparameter search for best model\r\n",
        "gs_model = GridSearchCV(<Best Model>(),\r\n",
        "                          param_grid=log_reg_grid,\r\n",
        "                          cv=5,\r\n",
        "                          verbose=True)\r\n",
        "\r\n",
        "# Fit grid hyperparameter search model\r\n",
        "gs_model.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_MEeQG9E9vW"
      },
      "source": [
        "# Check the best hyperparmaters\r\n",
        "gs_model.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R41Q8X-sE_LY"
      },
      "source": [
        "# Evaluate the grid search model model\r\n",
        "gs_model.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU9mwKUaGeJN"
      },
      "source": [
        "# ***Predictions***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ4h9AGDGher"
      },
      "source": [
        "test_preds = model.predict(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ84jQtoG4HN"
      },
      "source": [
        "# ***Features Importance***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbZX6zcxG7BW"
      },
      "source": [
        "# Find features importance of our best model\r\n",
        "model.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK1HmKUKG-_g"
      },
      "source": [
        "# Helper function for plotting feature importance \r\n",
        "def plot_features(columns, importances, n=20):\r\n",
        "  df = (pd.DataFrame({\"features\": columns,\r\n",
        "                      \"feature_importances\": importances})\r\n",
        "        .sort_values(\"feature_importances\", ascending=False)\r\n",
        "        .reset_index(drop=True))\r\n",
        "  \r\n",
        "  # Plot the dataframe\r\n",
        "  fig, ax = plt.subplots()\r\n",
        "  ax.barh(df[\"features\"][:n], df[\"feature_importances\"][:n])\r\n",
        "  ax.set_ylabel(\"Features\")\r\n",
        "  ax.set_xlabel(\"Feature Importance\")\r\n",
        "  ax.invert_yaxis()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H3gNy6lHR-N"
      },
      "source": [
        "# Match coef's of features to columns\r\n",
        "feature_dict = dict(zip(df.columns, list(model.coef_[0])))\r\n",
        "feature_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHGiH44THPPb"
      },
      "source": [
        "# Visualize feature importance\r\n",
        "feature_df = pd.DataFrame(feature_dict, index=[0])\r\n",
        "feature_df.T.plot.bar(title = \"Feature Importance\", legend=False);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}