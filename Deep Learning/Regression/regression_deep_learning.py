# -*- coding: utf-8 -*-
"""Regression Deep learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HdIvUNd51cjExqfZTRl-Ikt_slgrRs2g
"""

import tensorflow as tf

# Other imports
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Get the data
!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv

# Load in the data
df = pd.read_csv('/content/moore.csv', header = None).values
X = df[:, 0].reshape(-1, 1) # Make it a 2-D array of size N x D where D = 1
y = df[:,1]

# Plot the data
plt.scatter(X, y)

# Since we want a linear model, let's take the log
y = np.log(y)
plt.scatter(X, y)

# Let's center the X data so the values are not too large
X = X - X.mean()

# Now create our TensforFlow model

model = tf.keras.models.Sequential([
                                    tf.keras.layers.Input( shape = (1,)),
                                    tf.keras.layers.Dense(1)
])

model.compile(optimizer = tf.keras.optimizers.SGD(0.001, 0.9), loss = 'mse')

# Learning rate scheduler 
def schedule(epoch, lr):
  if epoch >= 50:
    return  0.0001
  return 0.001

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)

# Train the model
r = model.fit(X, y, epochs = 200, callbacks= [scheduler])

# Plot the loss
plt.plot(r.history['loss'], label = "Loss")

# Get the slope of the line
# The slope of the line is related to the doubling rate of transistor count
print(model.layers) # Note: There is only one 1 layer, the 'Input' layer doesn't count
print(model.layers[0].get_weights())

# The slope of the line is:
a = model.layers[0].get_weights()[0][0,0]

"""Our original model for exponential growth is:

$$ C = A_0 r^t $$

Where $ C $ is transistor the count and $ t $ is the year.

$ r $ is the rate of growth. For example, when $ t $ goes from 1 to 2, $ C $ increases by a factor of $ r $. When $ t $ goes from 2 to 3, $ C $ increases by a factor of $ r $ again.

When we take the log of both sides, we get:

$$ \log C = \log r * t + \log A_0 $$

This is our linear equation:

$$ \hat{y} = ax + b $$

Where:

$$ \hat{y} = \log C $$
$$ a = \log r $$
$$ x = t $$
$$ b = \log A_0 $$

We are interested in $ r $, because that's the rate of growth. Given our regression weights, we know that:

$$ a = 0.34188038 $$

so that:

$$ r = e^{0.34188038} = 1.4076 $$

To find the time it takes for transistor count to double, we simply need to find the amount of time it takes for $ C $ to increase to $ 2C $.

Let's call the original starting time $ t $, to correspond with the initial transistor count $ C $.

Let's call the end time $ t' $, to correspond with the final transistor count $ 2C $.

Then we also have:

$$ 2C = A_0 r ^ {t'} $$

Combine this with our original equation:

$$ C = A_0 r^t $$

We get (by dividing the 2 equations):

$$ 2C/C = (A_0 r ^ {t'}) / A_0 r^t $$

Which simplifies to:

$$ 2 = r^{(t' - t)} $$

Solve for $ t' - t $:

$$ t' - t = \frac{\log 2}{\log r} = \frac{\log2}{a}$$


Important note! We haven't specified what the starting time $ t $ actually is, and we don't have to since we just proved that this holds for any $ t $.
"""

print("Time to double:", np.log(2) / a)

X = np.array(X).flatten()
y = np.array(y)
denominator = X.dot(X) - X.mean() * X.sum()
a = (X.dot(y) - y.mean() * X.sum() / denominator)
b = ( y.mean() * X.dot(X) - X.mean() * X.dot(y)) / denominator
print(a,b)
print("Time to double:" , np.log(2) / a)