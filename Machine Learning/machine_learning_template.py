# -*- coding: utf-8 -*-
"""Machine Learning Template

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V5TOwrfSvWIE6WIAmvt1rYwIs26Cpgej

**< Title >**

**We're going to take the following approach:**
>1. Problem Defintion
2. Data
3. Evaluation
4. Features
5. Modelling
6. Experimentation

# ***1. Problem Defintion***

In a statement,
>***Explain the problem***

# ***2. Data***

*The original data came from < The dataset title> data from the < The website/company title >.*

< Dataset url >

## ***3. Evaluation***

*< Explain what you want to achieve>*

< Evaluation URL >

# ***4. Features***

*Some information about the data:*

*   
*   
*   
*
"""

# Import all the tools we need

# Regular EDA (Exploratory data analysis) and plotting libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Models from Scikit-Learn(Regression)
from sklearn.linear_model import LinearRegression 
from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import SVR
from skelearn.tree import DecisionTreeRegressor
from skelarn.ensemble import RandomForestClassifier

# Models from Scikit-Learn(Classification)
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from skelearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Model Evaluations(Regression)
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix

# Model Evaluations(Classification)
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix

"""### Load data"""

file_path = < File Path >
df = pd.read_csv(file_path)
df.shape # (rows, columns)

"""## Data Exploration (Exploratory data analysis or EDA)

The goal here is to find out more about the data and become a subject matter export on the dataset you're working with.



1.   What question(s) are you trying to solve?
2.   What kind of data do we have and how do we treat different types?
3.   What's missing from the data and how do you deal with it?
4.   Where are the outliers and why should you care about them?
5.   How can you add, change or remove features to get more out of your data?



"""

df.head()

df.tail()

df.describe()

df.info();

# Let's find out how many of each feature there
df["<Target>"].value_counts()

# Are there any missing values?
df.isna().sum()

# How many images are there of each label?(Classification)
labels_csv.<target>.value_counts().plot.bar(figsize=(20,10));

# What's the median number of labels per class?(Classification)
labels_csv.breed.value_counts().median()

# Compare target column with <feature> column
pd.crosstab(df.<target>, df.<feature>)

# Create a plot of crosstab
pd.crosstab(df.<target>,df.<feature>).plot(kind="bar",color=["salmon","lightblue"])
plt.title("")
plt.xlabel("<Feature labels>")
plt.ylabel("Amount")
plt.legend(["<Featur labels>]);
plt.xticks(rotation=0);

# Correlation matrix a little prettier
corr_matrix = df.corr()
fix, ax = plt.subplots(figsize= (15,10))
ax = sns.heatmap(corr_matrix, annot=True, fmt=".2f",cmap="YlGnBu");

"""## ***Make a copy of the original DataFrame***"""

#Make a copy
df_tmp = df.copy()

"""## ***Convert string to categories***"""

# Find the columns which contain string
for label, content in df_tmp.items():
  if pd.api.types.is_string_dtype(content):
    print(label)

# This will turn all of the string values into category values
for label, content in df_tmp.items():
  if pd.api.types.is_string_dtype(content):
    df_tmp[label] = content.astype("category").cat.as_ordered()

df_tmp.info()

df_tmp.<Feature>.cat.categories

df_tmp.<Feature>.cat.codes

#check missing data
df_tmp.isnull().sum()/len(df_tmp)

"""## ***Save preprocessed data***"""

# Export current tmp dataframe
df_tmp.to_csv("/content/train_tmp.csv",index=False)

df.head()

df.tail()

"""## ***Fill missing values***

###   ***Fill numerical missing values***
"""

for label, content in df_tmp.items():
  if pd.api.types.is_numeric_dtype(content):
    print(label)

# Check for which numeric columns have null values
for label, content in df_tmp.items():
  if pd.api.types.is_numeric_dtype(content):
    if pd.isnull(content).sum():
      print(label)

# Fill numeric rows with the median 
for label, content in df_tmp.items():
  if pd.api.types.is_numeric_dtype(content):
    if pd.isnull(content).sum():
      # Add a binary column which tells us if the data was missing or not
      df_tmp[label + "_is_missing"] = pd.isnull(content)
      # Fill missing numeric values with median
      df_tmp[label] = content.fillna(content.median())

# Check if there is any null numeric values
for labels, content in df_tmp.items():
  if pd.api.types.is_numeric_dtype(content):
    if pd.isnull(content).sum():
      print(label)

# Check to see how many examples were missing
df_tmp.auctioneerID_is_missing.value_counts()

df_tmp.isna().sum()

df_tmp.head()

df_tmp.info()

"""### ***Fill and turning categorical variables into numbers***"""

# Check for columns which aren't numeric
for label, content in df_tmp.items():
  if not pd.api.types.is_numeric_dtype(content):
    print(label)

# Turn categorical variables into numbers 
for label, content in df_tmp.items():
  if not pd.api.types.is_numeric_dtype(content):
    #Add a binary column to indicate whether sample had missing value
    df_tmp[label + "_is_missing"] = pd.isnull(content)
    #Turn categories into numbers and add +1
    df_tmp[label] = pd.Categorical(content).codes + 1

df_tmp.head()

df_tmp.isna().sum()

"""# ***Instantiate model***

## ***Splitting data into train/validation sets***
"""

# Split data into X & y 
df_X, df_y = df_tmp.drop("<Target>", axis=1), df_tmp.<Target>

# Split data into train and test sets
X_train,X_valid,y_train,y_valid = train_test_split(df_X,df_y,test_size=0.2)

"""## **Building an evaluation functions**"""

def evaluation_function(y_test,y_preds):
  return <Evaluation_Target>(y_test,y_preds)

# Create function to evaluate model on a few different levels(Regression)
def show_scores(model):
  train_preds = model.predict(X_train)
  val_preds = model.predict(X_valid)
  scores = {"Training MAE": mean_absolute_error(y_train,train_preds),
            "Valid MAE": mean_absolute_error(y_valid,val_preds),
            "Training <Evaluation Target>": evaluation_function(y_train, train_preds),
            "Valid <Evaluation Target>": evaluation_function(y_valid, val_preds),
            "Training R^2": r2_score(y_train, train_preds),
            "Valid R^2": r2_score(y_valid, val_preds)}
  return scores

# Create functions to evaluate model on a few different levels(Classification)
def plot_conf_mat(y_test, y_preds):
  """
  Plots a nice looking confusion matrix using Seaborn's heat map
  """
  fig, ax = plt.subplots()
  ax = sns.heatmap(confusion_matrix(y_test,y_preds),annot=True,cbar=False)
  plt.xlabel("True Label")
  plt.ylabel("Predicted Label")

print(classification_report(y_test,y_preds))

"""# ***Modelling***"""

# Put models in a dictionary
models = {"<Model Name>": <Model Function>(random_state=<Number>),
          "<Model Name>": <Model Function>(random_state=<Number>)}

# Create a function to fit and score models
def fit_and_score(models,X_train,X_test,y_train,y_test):
  """
  Fits and evaluates given machine learning models.
  models : a dict of differetn Scikit-Learn machine learning models
  X_train : training data (no labels)
  X_test : testing data (no labels)
  y_train : training labels
  y_test : test labels
  """

  # Make a dictionary to keep model scores
  model_scores = {}

  #Loop through models
  for name, model in models.items():
    # Fit the model to the data
    model.fit(X_train,y_train)
    # Evaluate the model and append its score to model_scores
    model_scores[name] = model.score(X_test, y_test)
  return model_scores

model_scores = fit_and_score(models, X_train,X_valid,y_train,y_valid)

model_scores

model_compare = pd.DataFrame(model_scores, index=["accuracy"])
model_compare.T.plot.bar();

"""## **Hyperparameter tuning with RandomizedSearchCV**


"""

# Different RandomForestRegressor hyperparameters
rf_grid = {"<Model Feature>": <Model Feature Inputs>(One or many(If many [])),
           "<Model Feature>": <Model Feature Inputs>(One or many(If many []))
           }

# Instantiate RandomizedSearchCV model
rs_model = RandomizedSearchCV(RandomForestRegressor(n_jobs=-1, random_state=42),
                              param_distributions=rf_grid,
                              n_iter=2, # Number of parameter settings that are sampled
                              cv=5, # Cross validation
                              verbose=True) # Controls the verbosity: the higher, the more messages

# Fit the RandomizedSearchCV model
rs_model.fit(X_train,y_train)

# Find the best hyperparameters
rs_model.best_params_

# Evaluate the randomized search best model
rs_model.score(X_valid,y_valid)

"""## ***Hyperparamter Tuning with GridSearchCV***

"""

# Different hyperparameters for our best model
model_grid = {"<Model Feature>": <Model Feature Inputs>(One or many(If many [])),
                "<Model Feature>": <Model Feature Inputs>(One or many(If many []))}

# Setup grid hyperparameter search for best model
gs_model = GridSearchCV(<Best Model>(),
                          param_grid=log_reg_grid,
                          cv=5,
                          verbose=True)

# Fit grid hyperparameter search model
gs_model.fit(X_train,y_train)

# Check the best hyperparmaters
gs_model.best_params_

# Evaluate the grid search model model
gs_model.score(X_test, y_test)

"""# ***Predictions***"""

test_preds = model.predict(df_test)

"""# ***Features Importance***"""

# Find features importance of our best model
model.feature_importances_

# Helper function for plotting feature importance 
def plot_features(columns, importances, n=20):
  df = (pd.DataFrame({"features": columns,
                      "feature_importances": importances})
        .sort_values("feature_importances", ascending=False)
        .reset_index(drop=True))
  
  # Plot the dataframe
  fig, ax = plt.subplots()
  ax.barh(df["features"][:n], df["feature_importances"][:n])
  ax.set_ylabel("Features")
  ax.set_xlabel("Feature Importance")
  ax.invert_yaxis()

# Match coef's of features to columns
feature_dict = dict(zip(df.columns, list(model.coef_[0])))
feature_dict

# Visualize feature importance
feature_df = pd.DataFrame(feature_dict, index=[0])
feature_df.T.plot.bar(title = "Feature Importance", legend=False);